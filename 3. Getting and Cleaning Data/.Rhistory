myapp
gtoken <- config(token = github_token)
gtoken
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
#    Replace your key and secret below.
myapp <- oauth_app("github", "3ab313538c301fa31716", "233ec0ebcdfb48f0bb0027f5c2ffef4df7876024")
#myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="00483298bc59aa29a03d6bdd0bcef5344f00b135")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# 4. Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
# convert unstructured json to structured json
library(jsonlite)
jsondata <- fromJSON(toJSON(content(req)))
# find out the created time of datasharing repo
subset(jsondata, name == "datasharing", select = c(created_at))
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="94649669c2f3a1f12c9a748dd8acdd0f83927687")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
install.packages("httpuv")
oauth_endpoints("github")
myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="94649669c2f3a1f12c9a748dd8acdd0f83927687")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token
library(httpuv)
myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="94649669c2f3a1f12c9a748dd8acdd0f83927687")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
if (!require("devtools")) install.packages("devtools")
devtools::install_github("rstudio/httpuv")
myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="94649669c2f3a1f12c9a748dd8acdd0f83927687")
myapp
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token
oauth_endpoints("github")
myapp <- oauth_app("github", key="43e20eb91a1056b92219", secret="94649669c2f3a1f12c9a748dd8acdd0f83927687")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
stop_for_status(req)
content(req)
acs
URL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
data<-read.csv(URL)
acs <- data
acs
head(acs)
str(acs)
install.packages("sqldf")
library(sqldf)
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select distinct AGEP from acs")
length(unique(acs$AGEP))
length(sqldf("select pwgtp1 from acs where AGEP < 50"))
nrow((sqldf("select pwgtp1 from acs where AGEP < 50")))
hurl <- "http://biostat.jhsph.edu/~jleek/contact.html"
con <- url(hurl)
htmlCode <- readLines(con)
close(con)
htmlCode
sapply(htmlCode[c(10, 20, 30, 100)], nchar)
nchar(htmlCode[10])
nchar(htmlCode[20])
nchar(htmlCode[100])
nchar(htmlCode[10])
nchar(htmlCode[20])
nchar(htmlCode[30])
nchar(htmlCode[100])
q5 <- read.fwf(file = "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for",
skip = 4,
widths = c(12, 7,4, 9,4, 9,4, 9,4))
sum(q5[, 4])
head(q5)
sum(q5[, 4])
library(mtcars)
head(mtcars)
mtcars$carname <- rownames(mtcars)
mtcars$carname
carMelt <- melt(mtcars, id=c("carname","gear","cyl"), measure.vars=c("mpg","hp"))
carMelt
cylData <- dcast(carMelt, cyl~varyable)
cylData <- dcast(carMelt, cyl~variable)
cylData <- dcast(carMelt, cyl ~ variable)
cylData
library(dplyr)
optins(wigth = 105)
options(wigth = 105)
chicago <- readRDS("chicago.rds")
dim(chicago)
chicago <- readRDS("chicago.rds")
swirl()
library()
swirl()
library(swirl)
swirl()
1
swirl()
mydf <- read.csv( path2csv , stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select(cran, ip_id, package, country)
cran [ , c(5:20)]
cran [c(5:20) ,]
info()
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
select(cran, country:r_arch)
select(cran, r_arch:country)
cran
select(cran, -time)
select(cran, -5:20)
-5:20
5:20
-(5:20)
select(cran, -(5:20))
select(cran, -(5:20))
select(cran, -(5:20))
select(cran, -(20:5))
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran,r_version == "3.1.1", country == "US")
?Comparison
filter(cran,r_version == "3.1.1", country == "IN")
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(r_version))
cran2<-select(cran , ip:id)
cran2<-select(cran , ip_id)
cran2<-select(cran , ip_id
cran2<-select(cran, size:ip_id)
arrange(cran2, ip_id)
desc(ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2,  country , desc(r_version),  ip_id )
cran3
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_gb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
swirl()
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by()
?group_by
by_package <- group_by(cran, package)
by_package
mean(size)
summarize(tbl_df)
by_package <- group_by(cran, package)
summarize(by_package , mean(size))
pack_sum <- summarize(by_package,
count = ,
unique = ,
countries = ,
avg_bytes = )
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
pack_sum
?n
submit()
submit()
pack_sum <- summarize(by_package,
count = ,
unique = ,
countries = ,
avg_bytes = )
pack_sum
submit()
reset()
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
pack_sum
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count>679)
top_counts
View(top_counts)
arrange(top_unique, desc(unique))
arrange(top_unique, desc(unique))
top_unique<-filter(pack_sum, unique > 465)
arrange(top_unique, desc(unique))
arrange(top_counts, desc(count))
top_counts_sorted <- arrange(top_counts, desc(count))
view(top_counts_sorted)
view()
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique<-filter(pack_sum, unique > 465)
View(top_unique)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
by_package <- group_by(cran, package)
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
top_countries <- filter(pack_sum, countries > 60)
result1 <- arrange(top_countries, desc(countries), avg_bytes)
print(result1)
submit()
result2 <-
arrange(
filter(
summarize(
group_by(cran,
package
),
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size)
),
countries > 60
),
desc(countries),
avg_bytes
)
print(result2)
submit()
submit()
View(result3)
cran %>%
select(ip_id, country, package, size) %>%
print
submit()
cran %>%
select(ip_id, country, package, size) %>%
mutate()
submit()
mutate(size_mb)
mutate(size_mb = size / 2^20)
submit()
print()
print(x)
print(cran)
submit()
submit()
filter(size_mb <= 0.5)
submit()
arrange(desc(size_mb)) %>%
print
submit()
swirl()
swirl()
install.packages(p, quiet = TRUE)
install.packages(tidyr, quiet = TRUE)
install.packages("tidyr", quiet = TRUE)
swirl()
library(swirl)
swirl()
library(tidyr)
students
?gather
gather (students,sex, count, -grade)
students2
res <- gather (students2,sex_class, count, -grade)
res
?separate
separate (data = res, col = sex_class, into = c("sex", "class"))
students2 %>%
gather(students2,sex_class, count, -grade) %>%
separate( col = sex_class , c("sex", "class")) %>%
print
submit()
students2 %>%
gather(sex_class, count, -grade) %>%
separate( col = sex_class , c("sex", "class")) %>%
print
submit()
students3
students3 %>%
gather(class, grade, class1:class5 ,na.rm = TRUE) %>%
print
submit()
?spread
students3 %>%
gather(class, grade, class1:class5, na.rm = TRUE) %>%
spread(test, grade) %>%
print
submit()
extract_numeric("class5")
students3 %>%
gather(class, grade, class1:class5, na.rm = TRUE) %>%
spread(test, grade) %>%
### Call to mutate() goes here %>%
mutate(class = extract_numeric(class)) %>% ### Call to mutate() goes here %>%
print
submit()
students4
student_info <- students4 %>%
select(id, name, sex) %>%
print
students4
submit()
submit()
submit()
passed
failed
passed <- mutate(passed, status = "passed")
passed
failed <- mutate(failed, status = "failed")
?bind_rows
bind_rows()
packageVersion('dplyr')
bind_rows(passed, failed)
sat
submit()
submit()
submit()
getwd()
URL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(URL, destfile = "./data/idaho.csv")
idaho<-read.csv("./data/idaho.csv")
str(idaho)
head(idaho)
nrow(idaho)
idaho[id := 1:length(idaho)]
select(id, ACR, AGS)
agricultureLogical <-
idaho %>%
mutate(id = 1:nrow(idaho)) %>%
select(id, ACR, AGS) %>%
filter(ACR >= 3& AGS >= 6) %>%
agricultureLogical <-
idaho %>%
mutate(id = 1:nrow(idaho)) %>%
select(id, ACR, AGS) %>%
filter(ACR >= 3& AGS >= 6) %>%
print
agricultureLogical
idaho %>%
mutate(id = 1:nrow(idaho)) %>%
select(id, ACR, AGS) %>%
filter(ACR >= 3& AGS >= 6) %>%
print
which(agricultureLogical)
agricultureLogical <- with(idaho, ACR == 3 & AGS ==6)
which(agricultureLogical)
which(agricultureLogical)[1:3]
library(jpeg)
install.packages("jpeg")
library(jpeg)
imgURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
imgFile <- tempfile()
download.file(imgURL, imgFile, method = "curl")
download.file(imgURL, imgFile)
img <- readJPEG(imgFile, native = TRUE)
download.file(imgURL, imgFile, mode = "wb")
img <- readJPEG(imgFile, native = TRUE)
img
quantile(img, probs = c(0.3, 0.8))
gdpURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
eduURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
gdpFile <- tempfile()
eduFile <- tempfile()
download.file(gdpURL, gdpFile)
download.file(eduURL, eduFile)
gdpData <- read.csv(gdpFile, skip = 5, nrows = 190, stringsAsFactors = F, header = F)
eduData <- read.csv(eduFile, stringsAsFactors = F)
gdpData
eduData
gdpData <- gdpData[, c(1, 2, 4, 5)]
colnames(gdpData) <- c("CountryCode", "Rank", "Country.Name", "GDP.Value")
gdpData$GDP.Value <- as.numeric(gsub(",", "", gdpData$GDP.Value))
matchedData <- merge(gdpData, eduData, by.x = "CountryCode", by.y = "CountryCode")
## Number of matched countries
dim(matchedData)[1]
library(plyr)
arrange(matchedData, desc(Rank))[13, 3]
mean(subset(matchedData, Income.Group %in% "High income: OECD", select = c(Rank))$Rank)
mean(subset(matchedData, Income.Group %in% c("High income: nonOECD","High income: OECD"), select = c(Rank))$Rank)
mean(subset(matchedData, Income.Group %in% "High income: nonOECD", select = c(Rank))$Rank)
breaks <- quantile(dt$rankingGDP, probs = seq(0, 1, 0.2), na.rm = TRUE)
breaks <- quantile(Rank$rankingGDP, probs = seq(0, 1, 0.2), na.rm = TRUE)
## This first line will likely take a few seconds. Be patient!
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
## calculate Emissions sum by year
aggTotals <- aggregate(Emissions ~ year,NEI, sum)
## barplot to .png
png('plot1.png')
barplot(
(aggTotals$Emissions)/10^6,
names.arg=aggTotals$year,
xlab="Year",
ylab="PM2.5 Emissions (10^6 Tons)",
main="Total PM2.5 Emissions From All US Sources"
)
dev.off()
# Histogram of the total number of steps taken each day
library(ggplot2)
library(lattice)
library(scales)
library(Hmisc)
Sys.setlocale("LC_TIME","English_United States.1252")
## Download, unzip and load data into data frame - activity
if(!file.exists("getdata-projectfiles-UCI HAR Dataset.zip")) {
temp <- tempfile()
download.file("http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip",temp)
unzip(temp)
unlink(temp)
}
activity <- read.csv("activity.csv")
# Histogram of the total number of steps taken each day
steps_day <- aggregate(steps ~ date, activity, sum , na.rm=TRUE)
hist(steps_day$steps, main = paste("Histogram of Total Steps By Day"), col="green", xlab="Number of Steps")
# Mean and median number of steps taken each day
mean(steps_day$steps)
median(steps_day$steps)
# Time series plot of the average number of steps taken
avgStepsPerTime <- aggregate(x=list(meanSteps=activity$steps), by=list(interval=activity$interval), FUN=mean, na.rm=TRUE)
ggplot(data=avgStepsPerTime, aes(x=interval, y=meanSteps)) +
geom_line() +
xlab("5-minute interval") +
ylab("average number of steps taken")
# The 5-minute interval that, on average, contains the maximum number of steps
maxSteps <- which.max(avgStepsPerTime$meanSteps)
timeMostSteps <-  avgStepsPerTime[maxSteps,'interval']
## Code to describe and show a strategy for imputing missing data
# 1.Calculate the total number of missing values in the dataset
NaCnt <- length(which(is.na(activity$steps)))
# 2.For filling the missing values in the dataset we could use the mean/median for that day
activityNAImputed <- activity
activityNAImputed$steps <- impute(activity$steps, fun=mean)
# Histogram of the total number of steps taken each day after missing values are imputed
stepsByDayImputed <- tapply(activityNAImputed$steps, activityNAImputed$date, sum)
qplot(stepsByDayImputed, xlab='Total Imputed steps per day', ylab='Frequency', binwidth=500)
# Panel plot comparing the average number of steps taken per 5-minute interval across weekdays and weekends
weekdays <- c("Monday", "Tuesday", "Wednesday", "Thursday",  "Friday")
activityNAImputed$dow = as.factor(ifelse(is.element(weekdays(as.Date(activityNAImputed$date)),weekdays), "Weekday", "Weekend"))
steps_by_interval_i <- aggregate(steps ~ interval + dow, activityNAImputed, mean)
xyplot(steps_by_interval_i$steps ~ steps_by_interval_i$interval|steps_by_interval_i$dow, main="Average Steps per Day by Interval",xlab="Interval", ylab="Steps",layout=c(1,2), type="l")
# All of the R code needed to reproduce the results (numbers, plots, etc.) in the report
library(knitr)
library(ggplot2)
library(datasets)
# clear environment variable
rm(list=ls(all=TRUE))
###  Exploratory Data Analysis
# Take a look at what the datasets consists of
head(mtcars)
summary(mtcars)
# Convert the categorical variables into factors
# We don't transform vs and am as factor because they only have a 2-state value
mtcars$cyl <- factor(mtcars$cyl)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
str(mtcars)
#The dataset has 32 observations of 11 variables. We will do a quick analysis on the variables
#to gain some insight on the distribution of mpg and the two modes of transmission.
summary(mtcars$mpg)
# We mainly focus on the relationship between variables mpg (Miles/(US) gallon) and am (Transmission). Box plot shows that there's a good separation of groups based on gas mileage.
boxplot(mpg ~ am, data = mtcars, col = "orange",  xlab = "Transmission (0 = automatic, 1 = manual)" , ylab="MPG",
main="Boxplot of MPG vs. Transmission")
#### Regression model
# Distribution of Manual and Automatic transmission Vehicles.
full_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(full_model, direction = "both")
## The best model obtained from the above computations consists of the variables, cyl, wt and hp as confounders and am as the independent variable. Details of the model are depicted below.
summary(best_model)
# From the above model details, we observe that the adjusted \(R^2\) value is 0.84 which is the maximum obtained considering all combinations of variables. Thus, we can conclude that more than 84% of the variability is explained by the above model.
# In the following section, we compare the base model with only am as the predictor variable and the best model which we obtained earlier containing confounder variables also.
base_model <- lm(mpg ~ am, data = mtcars)
anova(base_model, best_model)
# Looking at the above results, the p-value obtained is highly significant and we reject the null hypothesis that the confounder variables cyl, hp and wt donâ€™t contribute to the accuracy of the model.
## Residuals
par(mfrow = c(1, 1))
plot(best_model)
t.test(mpg ~ am, data = mtcars)
